# Pipeline de Datos - Causas Principales de Muerte en EE.UU.
## VersiÃ³n Simplificada (PostgreSQL + Python)

Un pipeline ETL eficiente y minimalista para anÃ¡lisis de las principales causas de muerte en Estados Unidos utilizando datos del NCHS (National Center for Health Statistics).

## ğŸ¯ Objetivo del Proyecto

Este proyecto implementa un pipeline ETL simplificado que:
- Procesa datos histÃ³ricos de mortalidad (1999-2017)
- Limpia y valida la calidad de los datos
- Crea un modelo de datos dimensional en PostgreSQL
- Genera reportes CSV automÃ¡ticos
- Mantiene una infraestructura mÃ­nima y eficiente

## ğŸ—ï¸ Arquitectura Simplificada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Datos CSV     â”‚â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚â”€â”€â”€â–¶â”‚   Reportes CSV  â”‚
â”‚   (NCHS)        â”‚    â”‚   (3 Esquemas)  â”‚    â”‚   & AnÃ¡lisis    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Pipeline Pythonâ”‚
                       â”‚   (Scripts)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes:

- **PostgreSQL 15**: Base de datos principal con 3 esquemas:
  - `raw_data`: Datos sin procesar
  - `clean_data`: Modelo dimensional (fact + dimensions)  
  - `analytics`: MÃ©tricas calculadas
- **Pipeline Python**: Scripts automatizados de procesamiento
- **Reportes CSV**: ExportaciÃ³n automÃ¡tica de resultados

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos
- Docker y Docker Compose
- Make (opcional pero recomendado)
- Al menos 2GB de RAM disponible

### 1. Clonar y configurar
```bash
# Crear estructura del proyecto
mkdir deaths-analysis && cd deaths-analysis

# Crear directorios necesarios
make create-dirs
# O manualmente:
mkdir -p data scripts init-db results config
```

### 2. Obtener los datos
```bash
# Colocar tu archivo CSV en la carpeta data/
cp tu_archivo.csv data/NCHS__Leading_Causes_of_Death__United_States.csv
```

### 3. Levantar la infraestructura
```bash
# OpciÃ³n 1: Usando Make (recomendado)
make setup

# OpciÃ³n 2: Docker Compose directo
docker-compose build
docker-compose up -d postgres
```

### 4. Ejecutar el pipeline
```bash
# Ejecutar pipeline completo
make pipeline

# Ver resultados
make view-results
```

## ğŸ“Š Servicio Disponible

| Servicio | URL | Credenciales |
|----------|-----|-------------|
| **PostgreSQL** | localhost:5432 | postgres / postgres123 |

## ğŸ—‚ï¸ Estructura del Proyecto

```
deaths-analysis/
â”œâ”€â”€ docker-compose.yml          # ConfiguraciÃ³n simplificada
â”œâ”€â”€ Dockerfile.python           # Imagen Python para pipeline
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ Makefile                    # AutomatizaciÃ³n de tareas
â”œâ”€â”€ README.md                   # Esta documentaciÃ³n
â”œâ”€â”€ .gitignore                  # Archivos a ignorar
â”‚
â”œâ”€â”€ data/                       # Datos fuente
â”‚   â””â”€â”€ NCHS__Leading_Causes_of_Death__United_States.csv
â”‚
â”œâ”€â”€ init-db/                    # Scripts de inicializaciÃ³n DB
â”‚   â””â”€â”€ 01-create-schema.sql
â”‚
â”œâ”€â”€ scripts/                    # Scripts Python
â”‚   â””â”€â”€ data_pipeline.py        # Pipeline principal
â”‚
â”œâ”€â”€ config/                     # Configuraciones opcionales
â”‚   â””â”€â”€ dev_config.json
â”‚
â””â”€â”€ results/                    # Resultados y reportes
    â”œâ”€â”€ national_trends.csv
    â”œâ”€â”€ state_analysis.csv
    â”œâ”€â”€ cause_trends.csv
    â””â”€â”€ pipeline_stats.json
```

## ğŸ”„ Flujo del Pipeline

### 1. **ExtracciÃ³n (Extract)**
- Carga automÃ¡tica del archivo CSV
- ValidaciÃ³n de estructura
- Logging de estadÃ­sticas iniciales

### 2. **TransformaciÃ³n (Transform)**
- Limpieza de nombres de columnas
- ValidaciÃ³n de tipos de datos
- DetecciÃ³n y correcciÃ³n de problemas:
  - Valores nulos o vacÃ­os
  - Duplicados
  - Valores fuera de rango
  - Outliers extremos

### 3. **Carga (Load)**
- **Raw Data**: `raw_data.deaths_raw`
- **Dimensional Model**:
  - `clean_data.dim_states`: DimensiÃ³n de estados
  - `clean_data.dim_causes`: DimensiÃ³n de causas
  - `clean_data.fact_deaths`: Tabla de hechos principal
- **Analytics**: `analytics.yearly_summary`

### 4. **Reportes**
- ExportaciÃ³n automÃ¡tica a CSV
- EstadÃ­sticas del pipeline
- Logs detallados de ejecuciÃ³n

## ğŸ“ˆ Reportes Generados

### Archivos CSV AutomÃ¡ticos
1. **national_trends.csv**: Tendencias nacionales por aÃ±o y causa
2. **state_analysis.csv**: AnÃ¡lisis comparativo por estado  
3. **cause_trends.csv**: EvoluciÃ³n temporal de cada causa
4. **pipeline_stats.json**: EstadÃ­sticas de calidad y ejecuciÃ³n

## ğŸ› ï¸ Comandos Principales

### GestiÃ³n de Servicios
```bash
make up              # Levantar PostgreSQL
make down            # Detener servicios  
make status          # Ver estado
make logs            # Ver logs
```

### Pipeline de Datos
```bash
make pipeline        # Ejecutar pipeline completo
make check-data      # Verificar archivo CSV
make view-results    # Mostrar resumen de resultados
make quick-analysis  # AnÃ¡lisis rÃ¡pido desde BD
```

### Base de Datos
```bash
make db-connect      # Conectar a PostgreSQL
make db-query        # SesiÃ³n interactiva SQL
make db-backup       # Crear backup
make db-reset        # Resetear BD
```

### AnÃ¡lisis y ExportaciÃ³n
```bash
make export-data     # Exportar dataset completo
make quick-analysis  # EstadÃ­sticas rÃ¡pidas
```

## ğŸ“Š Estructura de la Base de Datos

### Esquema Raw Data
```sql
raw_data.deaths_raw (
    id SERIAL PRIMARY KEY,
    year INTEGER,
    cause_name_113 TEXT,
    cause_name TEXT, 
    state TEXT,
    deaths INTEGER,
    age_adjusted_death_rate DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
```

### Esquema Clean Data (Modelo Dimensional)
```sql
-- Tabla de Hechos Principal
clean_data.fact_deaths (
    death_id SERIAL PRIMARY KEY,
    year INTEGER NOT NULL,
    state_id INTEGER REFERENCES dim_states(state_id),
    cause_id INTEGER REFERENCES dim_causes(cause_id), 
    deaths INTEGER NOT NULL,
    age_adjusted_death_rate DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)

-- DimensiÃ³n Estados
clean_data.dim_states (
    state_id SERIAL PRIMARY KEY,
    state_name VARCHAR(100) NOT NULL UNIQUE,
    is_national_aggregate BOOLEAN DEFAULT FALSE,
    state_code CHAR(2),
    region VARCHAR(50)
)

-- DimensiÃ³n Causas
clean_data.dim_causes (
    cause_id SERIAL PRIMARY KEY,
    cause_name VARCHAR(200) NOT NULL UNIQUE,
    cause_name_113 TEXT,
    cause_category VARCHAR(100),
    is_all_causes BOOLEAN DEFAULT FALSE
)
```

### Esquema Analytics
```sql
analytics.yearly_summary (
    summary_id SERIAL PRIMARY KEY,
    year INTEGER NOT NULL,
    total_deaths INTEGER,
    avg_death_rate DECIMAL(10,2),
    leading_cause VARCHAR(200),
    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
```

## ğŸ” Validaciones de Calidad

### Controles AutomÃ¡ticos
- âœ… Tipos de datos correctos
- âœ… Campos crÃ­ticos no nulos  
- âœ… Rangos vÃ¡lidos de aÃ±os (1990-2030)
- âœ… Valores no negativos en muertes
- âœ… DetecciÃ³n de outliers (percentil 99)
- âœ… EliminaciÃ³n de duplicados exactos
- âœ… Integridad referencial

### MÃ©tricas Reportadas
- Registros procesados vs. removidos
- Completitud por aÃ±o
- Estados y causas identificados
- Tiempo de ejecuciÃ³n del pipeline

## ğŸš¨ SoluciÃ³n de Problemas

### PostgreSQL No Inicia
```bash
make logs            # Ver logs de error
make db-reset        # Resetear si es necesario
docker system prune  # Limpiar recursos
```

### Archivo de Datos No Encontrado
```bash
# Verificar ubicaciÃ³n exacta
ls -la data/
# Debe existir: data/NCHS__Leading_Causes_of_Death__United_States.csv
```

### Pipeline Falla  
```bash
make logs-pipeline   # Ver logs detallados
make test-connection # Probar conexiÃ³n DB
make pipeline-dev    # Ejecutar con configuraciÃ³n de desarrollo
```

### Ver Estado General
```bash
make status          # Estado de servicios
make monitor         # Monitoreo en tiempo real
make quick-analysis  # Verificar datos en BD
```

## ğŸ”§ PersonalizaciÃ³n

### ConfiguraciÃ³n Personalizada
```bash
# Crear configuraciÃ³n personalizada
cp config/dev_config.json config/mi_config.json
# Editar segÃºn necesidades

# Usar configuraciÃ³n personalizada
docker-compose run --rm python_pipeline python scripts/data_pipeline.py --config /app/config/mi_config.json
```

### Variables de Entorno
```bash
# En docker-compose.yml, secciÃ³n python_pipeline:
environment:
  - DB_HOST=postgres
  - DB_NAME=deaths_analysis  
  - DB_USER=postgres
  - DB_PASSWORD=tu_password_personalizada
```

### AÃ±adir Nuevos AnÃ¡lisis
```bash
# Conectar y ejecutar SQL personalizado
make db-query

# Exportar resultados personalizados
make db-connect
# Luego: \copy (SELECT ...) TO '/results/mi_reporte.csv' WITH CSV HEADER;
```

## ğŸ“š Comandos SQL Ãštiles

### Exploratorios BÃ¡sicos
```sql
-- Conectar con: make db-connect

-- Ver estructura de tablas
\dt clean_data.*

-- Resumen rÃ¡pido  
SELECT COUNT(*) FROM clean_data.fact_deaths;
SELECT COUNT(*) FROM clean_data.dim_states;  
SELECT COUNT(*) FROM clean_data.dim_causes;

-- Top causas de muerte
SELECT dc.cause_name, SUM(fd.deaths) as total_deaths
FROM clean_data.fact_deaths fd
JOIN clean_data.dim_causes dc ON fd.cause_id = dc.cause_id
WHERE dc.is_all_causes = false
GROUP BY dc.cause_name
ORDER BY total_deaths DESC;
```

## âš¡ Rendimiento

### Optimizaciones Incluidas
- Ãndices automÃ¡ticos en campos clave
- Carga por lotes (mÃ©todo `multi`)
- Conexiones pooled con SQLAlchemy
- ValidaciÃ³n eficiente de datos

### Para Datasets Grandes
```bash
# Usar configuraciÃ³n optimizada
echo '{"processing": {"validate_data": false}}' > config/fast_config.json
docker-compose run --rm python_pipeline python scripts/data_pipeline.py --config /app/config/fast_config.json
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama: `git checkout -b feature/mi-mejora`
3. Commit: `git commit -m 'Agregar nueva funcionalidad'`  
4. Push: `git push origin feature/mi-mejora`
5. Abre un Pull Request

## ğŸ“„ Licencia

Proyecto bajo Licencia MIT - ver [LICENSE](LICENSE) para detalles.

## ğŸ†˜ Soporte

Para problemas o preguntas:

1. Revisar [SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
2. Ejecutar `make logs` para diagnÃ³sticos
3. Usar `make help` para ver todos los comandos
4. Abrir issue en el repositorio

---

**Pipeline Simplificado y Eficiente! ğŸš€ğŸ“Š**